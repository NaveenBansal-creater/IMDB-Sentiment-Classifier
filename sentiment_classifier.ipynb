{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "%matplotlib inline\n",
    "\n",
    "import process_text\n",
    "import split_data\n",
    "import IMDB_dataset\n",
    "import model\n",
    "import training\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load reviews\n",
    "pos_path = Path('/data/analytics/naveen.bansal/pytorch/data/aclImdb/train/pos')\n",
    "neg_path = Path('/data/analytics/naveen.bansal/pytorch/data/aclImdb/train/neg')\n",
    "\n",
    "pos = [open(x).read() for x in pos_path.iterdir() if x.is_file()]\n",
    "neg = [open(x).read() for x in neg_path.iterdir() if x.is_file()]\n",
    "labels = [1]*len(pos) + [0]*len(neg)\n",
    "reviews = pos+neg\n",
    "df = pd.DataFrame(columns=('review_raw','label'))\n",
    "df['review_raw'] = reviews\n",
    "df['label'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review_processed'] = df['review_raw'].apply(lambda x: process_text.preprocess_text(x))\n",
    "vocab_to_int = process_text.get_vocab_to_int(df['review_processed'].tolist())\n",
    "df['review_encoded'] = df['review_processed'].apply(lambda x: process_text.encode_sent(x,vocab_to_int))\n",
    "df['review_encoded_padded'] = df['review_encoded'].apply(lambda x: process_text.pad_features(x,500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_raw</th>\n",
       "      <th>label</th>\n",
       "      <th>review_processed</th>\n",
       "      <th>review_encoded</th>\n",
       "      <th>review_encoded_padded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Zentropa has much in common with The Third Man...</td>\n",
       "      <td>1</td>\n",
       "      <td>zentropa much common third man another noirlik...</td>\n",
       "      <td>[12826, 13, 992, 718, 49, 64, 42920, 3, 170, 6...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Zentropa is the most original movie I've seen ...</td>\n",
       "      <td>1</td>\n",
       "      <td>zentropa original movie ive seen years like un...</td>\n",
       "      <td>[12826, 101, 2, 97, 33, 59, 5, 813, 3020, 3976...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lars Von Trier is never backward in trying out...</td>\n",
       "      <td>1</td>\n",
       "      <td>lars von trier never backward trying new techn...</td>\n",
       "      <td>[8274, 2422, 7163, 35, 11089, 156, 65, 3257, 1...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>*Contains spoilers due to me having to describ...</td>\n",
       "      <td>1</td>\n",
       "      <td>contains spoilers due describe film techniques...</td>\n",
       "      <td>[1161, 1065, 543, 1439, 3, 3257, 214, 35975, 1...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>That was the first thing that sprang to mind a...</td>\n",
       "      <td>1</td>\n",
       "      <td>first thing sprang mind watched closing credit...</td>\n",
       "      <td>[20, 61, 31480, 241, 181, 2498, 783, 5494, 25,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          review_raw  label  \\\n",
       "0  Zentropa has much in common with The Third Man...      1   \n",
       "1  Zentropa is the most original movie I've seen ...      1   \n",
       "2  Lars Von Trier is never backward in trying out...      1   \n",
       "3  *Contains spoilers due to me having to describ...      1   \n",
       "4  That was the first thing that sprang to mind a...      1   \n",
       "\n",
       "                                    review_processed  \\\n",
       "0  zentropa much common third man another noirlik...   \n",
       "1  zentropa original movie ive seen years like un...   \n",
       "2  lars von trier never backward trying new techn...   \n",
       "3  contains spoilers due describe film techniques...   \n",
       "4  first thing sprang mind watched closing credit...   \n",
       "\n",
       "                                      review_encoded  \\\n",
       "0  [12826, 13, 992, 718, 49, 64, 42920, 3, 170, 6...   \n",
       "1  [12826, 101, 2, 97, 33, 59, 5, 813, 3020, 3976...   \n",
       "2  [8274, 2422, 7163, 35, 11089, 156, 65, 3257, 1...   \n",
       "3  [1161, 1065, 543, 1439, 3, 3257, 214, 35975, 1...   \n",
       "4  [20, 61, 31480, 241, 181, 2498, 783, 5494, 25,...   \n",
       "\n",
       "                               review_encoded_padded  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train, validation, test sets\n",
    "df_train, df_val, df_test = \\\n",
    "    split_data.split_stratified_into_train_val_test(df, stratify_colname='label', frac_train=0.70, frac_val=0.29, frac_test=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dataset and dataloader\n",
    "batch_size=64\n",
    "train_loader = IMDB_dataset.get_dataloader(df_train,'review_encoded_padded','label',batch_size=batch_size)\n",
    "val_loader   = IMDB_dataset.get_dataloader(df_val,'review_encoded_padded','label',batch_size=batch_size)\n",
    "test_loader  = IMDB_dataset.get_dataloader(df_test,'review_encoded_padded','label',batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UserActivityModel(\n",
      "  (embedding): Embedding(117846, 50)\n",
      "  (lstm): LSTM(50, 256, num_layers=2, batch_first=True, dropout=0.3)\n",
      "  (dense): Linear(in_features=256, out_features=2, bias=True)\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the LSTM model w/ hyperparams\n",
    "n_vocab = len(vocab_to_int)+1 # +1 for the 0 padding\n",
    "output_size = 2\n",
    "embedding_size = 50\n",
    "hidden_state_size = 256\n",
    "num_layers = 2\n",
    "net = model.UserActivityModel(n_vocab, embedding_size, num_layers, hidden_state_size,output_size, drop_prob=0.3,use_gpu=True)\n",
    "net.to(device)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss and optimization functions\n",
    "lr=0.001\n",
    "epochs=10\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "tb = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06:20:07.656452:Epoch:0 0.6898624739804111 0.5250686813186813 0.6894782159180768 0.5460453539823009\n",
      "06:20:32.825113:Epoch:1 0.6599879366355937 0.6073145604395604 0.5947488232523994 0.6966261061946902\n",
      "06:20:55.951204:Epoch:2 0.5459687084287077 0.7539491758241759 0.52079620614516 0.781941371681416\n",
      "06:21:19.551697:Epoch:3 0.47462257502716537 0.8325892857142857 0.49033811029079744 0.8151272123893806\n",
      "06:21:43.029635:Epoch:4 0.4393374134987702 0.8696771978021978 0.5180963451883435 0.7876106194690266\n",
      "06:22:06.357882:Epoch:5 0.41631144274285425 0.8942307692307693 0.468198812113399 0.8389103982300885\n",
      "06:22:32.132458:Epoch:6 0.40540621830866885 0.9057921245421245 0.46618670726244427 0.8402931415929203\n",
      "06:22:57.208441:Epoch:7 0.4015399124814477 0.909283424908425 0.4657689609886271 0.8419524336283186\n",
      "06:23:20.734009:Epoch:8 0.3784359201188489 0.9339514652014652 0.4632963886303184 0.8462389380530974\n",
      "06:23:45.940905:Epoch:9 0.3763459949266343 0.9362408424908425 0.46645842795878384 0.8419524336283186\n"
     ]
    }
   ],
   "source": [
    "# train and validation loop\n",
    "training.train(tb,epochs,net,train_loader,val_loader,batch_size,optimizer,criterion,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.4947056869665782, Test Accuracy: 0.8125\n"
     ]
    }
   ],
   "source": [
    "# test set accuracy\n",
    "net.eval()\n",
    "test_accuracy=[]\n",
    "test_losses=[]\n",
    "for batch_indx,data in enumerate(test_loader):\n",
    "\n",
    "    hidden = net.zero_state(batch_size)\n",
    "\n",
    "    inputs, labels = data['x'].to(device), data['y'].to(device)\n",
    "\n",
    "    out = net.forward(inputs, hidden)\n",
    "\n",
    "    loss = criterion(out,labels.flatten())\n",
    "    test_losses.append(loss.item())\n",
    "\n",
    "    y_pred = torch.argmax(out,dim=1)\n",
    "    accuracy = (y_pred==labels.long().squeeze()).sum().item()/y_pred.shape[0]\n",
    "    test_accuracy.append(accuracy)\n",
    "print (f\"Test loss: {np.mean(test_losses)}, Test Accuracy: {np.mean(test_accuracy)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_text(text):\n",
    "    #word_seq = np.array([vocab[preprocess_string(word)] for word in text.split() \n",
    "    #                 if preprocess_string(word) in vocab.keys()])\n",
    "    #word_seq = np.expand_dims(word_seq,axis=0)\n",
    "    #pad =  torch.from_numpy(padding_(word_seq,500))\n",
    "    review_processed = process_text.preprocess_text(text)\n",
    "    review_encoded = process_text.encode_sent(review_processed,vocab_to_int)\n",
    "    review_encoded_padded = process_text.pad_features(review_encoded,500)\n",
    "    \n",
    "    review_encoded_padded = torch.tensor([review_encoded_padded],dtype=torch.long).reshape(1,-1)\n",
    "    #print (review_encoded_padded)\n",
    "    inputs = review_encoded_padded.to(device)\n",
    "    batch_size = 1\n",
    "    net.eval()\n",
    "    hidden = net.zero_state(batch_size)\n",
    "    #h = tuple([each.data for each in h])\n",
    "    out = net.forward(inputs, hidden)\n",
    "    #print (out)\n",
    "    return([out[0][0].item(),out[0][1].item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mimicking its long title the movie finds ways to come close to the 90' mark. The beautiful sets are here with all that made the Hamer production values a trademark, yet Paris drowned in the fog is a sign of indolent neglect. The story is obvious and can be summed up in a dozen words so there comes nothing unexpected and nothing worth more than 5% of your attention to be expected.<br /><br />The directing is heavy as a direct transfer from the stage play, actors are mostly stiff as wax figures (ok this is a Hamer feature, only it's sometimes better featured in the whole package). My conclusion: this movie is trash, not worth the time I spend that evening. Eternal life is a boring matter and I should have hoped the guys in charge of programming at the CinemathÃ¨que would have known better.\n",
      "======================================================================\n",
      "Actual sentiment is  : 0\n",
      "======================================================================\n",
      "[0.9893736243247986, 0.010626359842717648]\n",
      "Predicted sentiment is negative with a probability of 0.9893736243247986\n"
     ]
    }
   ],
   "source": [
    "index = 195\n",
    "print(df_test['review_raw'][index])\n",
    "print('='*70)\n",
    "print(f'Actual sentiment is  : {df_test[\"label\"][index]}')\n",
    "print('='*70)\n",
    "pro = predict_text(df_test['review_raw'][index])\n",
    "print (pro)\n",
    "status = \"negative\" if pro[0] > 0.5 else \"positive\"\n",
    "pro = pro[0] if status == \"negative\" else pro[1]\n",
    "print(f'Predicted sentiment is {status} with a probability of {pro}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
